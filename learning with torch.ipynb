{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d5d7b4-e0a4-490b-ac1c-2d88e426a634",
   "metadata": {},
   "source": [
    "This is another attempt using tutorial from https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486608c-5659-47e7-9635-913baee3e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149982c-e44f-4d94-94cd-bb05f78df49f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Testing $D_{KL}$ from torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969b04f-641c-4349-8b7e-40f09f4c85d9",
   "metadata": {},
   "source": [
    "Theoretical value for kl divergence between two univariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ddae6bd-443a-4250-b533-36ebf0070124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_theory(mean1,sd1,mean2,sd2):\n",
    "    return np.log(sd2/sd1)+(sd1*sd1+(mean1-mean2)*(mean1-mean2))/(2*sd2*sd2)-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4936794-f5a6-422e-8707-16b195791869",
   "metadata": {},
   "source": [
    "Testing the function with random values where $\\mu \\in (-100,100)$ and $\\sigma \\in (0,100)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb8910d5-3de4-4c4f-bcc6-a5a4aa07246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Difference = 0.00057\n"
     ]
    }
   ],
   "source": [
    "#this function seems ok\n",
    "sq_diff = 0\n",
    "for k in range(1000):\n",
    "    (mu1, mu2, sd1, sd2)= (np.random.random_sample()*200-100, \n",
    "                           np.random.random_sample()*200-100,\n",
    "                           np.random.random_sample()*100,\n",
    "                           np.random.random_sample()*100)\n",
    "    test_val = torch.distributions.kl.kl_divergence(\n",
    "        torch.distributions.Normal(mu1,sd1),\n",
    "        torch.distributions.Normal(mu2,sd2))\n",
    "    theory_val = kl_theory(mu1,sd1,mu2,sd2)\n",
    "    sq_diff = sq_diff + (theory_val-test_val)*(theory_val-test_val)\n",
    "    \n",
    "mean_sq_diff = sq_diff/1000\n",
    "print('Mean Squared Difference = %1.5f' %mean_sq_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c4feb-5849-4401-a6d5-8260f8e32032",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## KL divergence from the website "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfba3ba-1d30-4382-8dd0-257327d4013a",
   "metadata": {},
   "source": [
    "(probably not gonna use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3016920-1ae2-4fd9-acd2-b6010837b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16445f3-68cb-4ce3-a229-875487b440ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensor learning/investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ada2c8-2c27-415b-86ee-475e57b251c7",
   "metadata": {},
   "source": [
    "Turning data from truth into tensor that we can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4dc683b7-1ce7-4343-928b-a9a0df0c0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(508)\n",
    "dat = np.random.normal(0,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ada45d0-e012-4813-a15a-fc239a4f5345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  3.6381,  -1.6319,  -6.1138,  -1.6868,   1.8680,  -1.2100, -10.4189,\n",
      "          7.6481,   3.7290,   4.0272], dtype=torch.float64)\n",
      "tensor([ 15.5522,  -5.5275, -23.4553,  -5.7472,   8.4719,  -3.8400, -40.6756,\n",
      "         31.5922,  15.9160,  17.1087], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "sample = torch.tensor(dat)\n",
    "print(sample)\n",
    "print(torch.add(4*sample,torch.ones_like(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d82130-da8e-41c1-aeeb-ca94ef5b06b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments with $q(z|x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "589838ca-9492-4ac1-8c51-d6c9d892725c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25b7e95ad90>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(508)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fa896a12-e7d7-4d2c-9e23-dba8d4d3f902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p=torch.distributions.Normal(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "054299e4-46d9-483f-b4c2-c76e89f9239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qzx=torch.distributions.Normal(loc=torch.add(4*sample,torch.ones_like(sample)),\n",
    "                               scale=torch.ones_like(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "90dfee08-3fea-4686-8f2e-87a9dc3816d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = qzx.rsample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933ae51-7146-4e45-bfa5-987e614037cf",
   "metadata": {},
   "source": [
    "Sampling from $q$ will generate a sample for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "14186268-ca7e-4122-82bb-026b0ce149d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 12.8077,  -5.7186, -22.4107,  -7.1926,   9.2840,  -1.8070, -39.1750,\n",
      "         31.7305,  16.7579,  17.5327], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0881a01-b8a8-4461-98f1-e92e1d6c2491",
   "metadata": {},
   "source": [
    "We can sum in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "61c9341a-b420-4bd5-b874-964dd247aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2070.3623, dtype=torch.float64)\n",
      "tensor([  -43.0605,    -4.5853,    -7.9904, -1022.7177,    -2.6962,  -654.3091,\n",
      "         -146.5263,  -634.6454,    -1.5597,   -41.3007], dtype=torch.float64)\n",
      "tensor(-2559.3913, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.distributions.kl.kl_divergence(qzx,p).sum(-1))\n",
    "print(q.log_prob(z))\n",
    "print(q.log_prob(z).sum(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3aff4a-69df-4dca-a5d2-164447eb531a",
   "metadata": {},
   "source": [
    "We can probably work out $D_{KL}$ this way, but <b>does it make sense to sum over the divergences</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e5542-105a-443d-a9a2-ec1dea47d0d9",
   "metadata": {},
   "source": [
    "## Working out expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de262e0-5298-430e-b5da-b785560bd28e",
   "metadata": {},
   "source": [
    "The examples from the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34392fc-5ac7-44da-b39c-eaa10af17879",
   "metadata": {},
   "source": [
    "They first define the likelihood function where the mean is parametrized by <code>x_hat</code> (apparently a sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacba73d-201e-4e42-952e-2436323bcdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "        scale = torch.exp(logscale)\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "        # measure prob of seeing image under p(x|z)\n",
    "        log_pxz = dist.log_prob(x)\n",
    "        return log_pxz.sum(dim=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d9893-3b49-4d96-ba15-e14de0ed22f3",
   "metadata": {},
   "source": [
    "Then they simply define $\\mathbb{E}_{q(z|x)}\\log p(x|z)$ (called recon_loss here) using <code>gaussian_likelihood</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f8301-d64d-46f2-9047-b8277ed2cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e44fc-bfa8-4010-9ad8-909624630152",
   "metadata": {
    "tags": []
   },
   "source": [
    "Several problems:\n",
    "- gaussian_likelihood simply sum over the log probabilities of $p(x|z)$, how does that calculate the expectation when the whole thing is given by $\\int q(z|x)\\log p(x|z) dz$?\n",
    "- the function only sum over finitely many points, how does that calculate the integration accurately?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8315584-dc64-4ad8-a3ea-4bba786b3e26",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "<ol>\n",
    "<li>there is a integration option using <code>torch.trapz</code>, but implementing might be difficult</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63eb19-06e7-4ab0-80ec-2a6a0898d603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
